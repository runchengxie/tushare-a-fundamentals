#!/usr/bin/env python3
"""Generate dataset field lists by parsing bundled TuShare API docs.

This script scans the Markdown files under ``docs/`` and extracts the output
field tables so we can keep our default ``fields`` selections in sync with the
official documentation without hand-maintaining long comma-separated strings.

Usage
-----
    uv run tools/update_dataset_fields.py

The script overwrites ``src/tushare_a_fundamentals/meta/doc_fields.py`` with the
latest mapping.  The generated file is pure Python (no runtime dependencies)
so regular imports remain fast.
"""

from __future__ import annotations

import argparse
import sys
from collections import OrderedDict
from pathlib import Path
from typing import Iterable, List, Sequence

ROOT = Path(__file__).resolve().parents[1]
DOCS_DIR = ROOT / "docs"
OUTPUT_PATH = ROOT / "src" / "tushare_a_fundamentals" / "meta" / "doc_fields.py"

# Map documentation filenames to DatasetSpec names.
DOC_DATASET_MAP = OrderedDict(
    [
        ("income_statement_tushare_api_doc.md", "income"),
        ("balance_sheet_tushare_api_doc.md", "balancesheet"),
        ("cash_flow_statement_tushare_api_doc.md", "cashflow"),
        ("earnings_preannouncement_tushare_api_doc.md", "forecast"),
        ("preliminary_unaudited_results_tushare_api_doc.md", "express"),
        ("dividend_info_tushare_api_doc.md", "dividend"),
        ("financial_ratios_tushare_api_doc.md", "fina_indicator"),
        ("audit_opinion_tushare_api_doc.md", "fina_audit"),
        ("revenue_breakdown_tushare_api_doc.md", "fina_mainbz"),
        ("release_date_tushare_api_doc.md", "disclosure_date"),
    ]
)


def _normalize_field(name: str) -> str | None:
    """Drop headers/separators and trim whitespace."""

    cleaned = name.strip()
    if not cleaned:
        return None
    lowered = cleaned.lower()
    if lowered in {"名称"}:
        return None
    if cleaned.startswith(":") or set(cleaned) <= {":", "-"}:
        return None
    return cleaned


def extract_output_fields(markdown: Iterable[str]) -> List[str]:
    """Extract the output field list from a TuShare Markdown document."""

    fields: List[str] = []
    collecting = False
    for raw_line in markdown:
        line = raw_line.strip()
        if not collecting:
            if line.startswith("**输出参数"):
                collecting = True
            continue
        if not line:
            if fields:
                break
            continue
        if not line.startswith("|"):
            if fields:
                break
            continue
        parts = [part.strip() for part in line.strip("|").split("|")]
        if not parts:
            continue
        normalized = _normalize_field(parts[0])
        if not normalized:
            continue
        if normalized not in fields:
            fields.append(normalized)
    if not fields:
        raise ValueError("未在文档中解析到输出字段表")
    return fields


HEADER = '''"""Auto-generated dataset field mapping.

This file is generated by ``tools/update_dataset_fields.py``. Do not edit by hand.
"""

from __future__ import annotations

DOC_FIELDS: dict[str, tuple[str, ...]] = {
'''


def _format_entry(name: str, fields: Sequence[str]) -> str:
    indent = " " * 4
    inner_indent = indent * 2
    body = ",\n".join(f'{inner_indent}"{item}"' for item in fields)
    return f'{indent}"{name}": (\n{body}\n{indent}),'


def write_mapping(mapping: OrderedDict[str, List[str]]) -> None:
    lines: List[str] = [HEADER.rstrip("\n")]
    entries = [_format_entry(dataset, fields) for dataset, fields in mapping.items()]
    lines.extend(entries)
    lines.append("}\n")
    OUTPUT_PATH.write_text("\n".join(lines), encoding="utf-8")


def main(argv: Sequence[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--check",
        action="store_true",
        help="仅校验当前文档能否成功解析，不写出文件。",
    )
    args = parser.parse_args(argv)

    mapping: OrderedDict[str, List[str]] = OrderedDict()
    for doc_name, dataset in DOC_DATASET_MAP.items():
        doc_path = DOCS_DIR / doc_name
        if not doc_path.exists():
            parser.error(f"文档不存在：{doc_path}")
        try:
            fields = extract_output_fields(doc_path.read_text(encoding="utf-8").splitlines())
        except ValueError as exc:
            parser.error(f"{doc_name} 解析失败：{exc}")
        mapping[dataset] = fields

    if args.check:
        for dataset, fields in mapping.items():
            print(f"{dataset}: {len(fields)} fields")
        return 0

    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    write_mapping(mapping)
    print(f"已写入字段映射：{OUTPUT_PATH.relative_to(ROOT)}")
    return 0


if __name__ == "__main__":
    sys.exit(main())

